{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 roc curves for distance thresholds and 1 table with roc auc and fscore for parameters epidemicmodel and ml classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X):\n",
    "    return np.array([[len(x), np.max(x), np.mean(x)] for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def human_human():\n",
    " pathlist=[]\n",
    " filelist=[]\n",
    " pr0='rss_smartphone-master/rss_smartphone-master'\n",
    " pr1= os.listdir(pr0)\n",
    " for x in pr1:\n",
    "     filelist.append(os.listdir(pr0+'/'+x))\n",
    "     pathlist.append(pr0+'/'+x)\n",
    "  \n",
    " filelist=np.array(filelist)\n",
    " traindata=[]\n",
    " for i in range(len(pathlist)):\n",
    "     datasetfile=filelist[:,2][i]\n",
    "     pathfile=pathlist[i]\n",
    "     with open(pathfile+'/'+datasetfile, newline='', encoding='ANSI') as f:\n",
    "          reader = csv.reader(f)\n",
    "          #traindata.append(list(reader))\n",
    "          traindata=traindata+list(reader)[1:]\n",
    " \n",
    " traindata=np.array(traindata)\n",
    " ## human_human test data load\n",
    " testdata=[]\n",
    " for i in range(len(pathlist)):\n",
    "     datasetfile=filelist[:,1][i]\n",
    "     pathfile=pathlist[i]\n",
    "     with open(pathfile+'/'+datasetfile, newline='', encoding='ANSI') as f:\n",
    "          reader = csv.reader(f)\n",
    "          #testdata.append(list(reader))\n",
    "          testdata=testdata+list(reader)[1:]\n",
    " testdata=np.array(testdata)\n",
    " ### human_human dataset vectors formation\n",
    " nts=500 # training samples\n",
    " shfindcs=np.arange(len(traindata))\n",
    " np.random.shuffle(shfindcs)\n",
    " X_train=traindata[:,2][shfindcs][:nts*int(len(traindata)/nts)].reshape(nts,int(len(traindata)/nts)).astype('int')\n",
    " traindist=traindata[:,5][shfindcs][:nts*int(len(traindata)/nts)].reshape(nts,int(len(traindata)/nts)).astype('int')\n",
    " ### human_human dataset vectors formation /test\n",
    " vlen=traindist.shape[1] # vector len\n",
    " shfindcs=np.arange(len(testdata))\n",
    " np.random.shuffle(shfindcs)\n",
    " X_test=testdata[:,2][shfindcs][:int(len(testdata)/vlen)*vlen].reshape(int(len(testdata)/vlen),vlen).astype('int')\n",
    " testdist=testdata[:,5][shfindcs][:int(len(testdata)/vlen)*vlen].reshape(int(len(testdata)/vlen),vlen).astype('int')\n",
    " cmthresh =75 #75\n",
    " reflen=100\n",
    " return cmthresh,reflen,testdist,traindist,X_test,X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nature dataset load\n",
    "\n",
    "def nature_data():\n",
    " datasetpath=\"ble-proximity-tracing-master/data/bundeswehr-04-01/\"\n",
    " # The reference Sequence, 600 secondes at a distance of 'cmthresh', for which we assume a contact is dangerous \n",
    " data_train = json.load(open(datasetpath+\"data_train.json\", \"r\"))\n",
    " X_train = [d[\"rss\"] for d in data_train]\n",
    " data_test = json.load( open(datasetpath+\"data_test.json\", \"r\"))\n",
    " X_test = [d[\"rss\"] for d in data_test] # for nature\n",
    " traindist=[d[\"dist\"] for d in data_train]\n",
    " testdist=[d[\"dist\"] for d in data_test]\n",
    " cmthresh =  225 #225\n",
    " reflen=600\n",
    " return cmthresh,reflen,testdist,traindist,X_test,X_train\n",
    "\n",
    "#y_train = [np.sum(epi_model(d[\"dist\"]))>THRESHOLD for d in data_train]  # for nature\n",
    "#y_test = [np.sum(epi_model(d[\"dist\"]))>THRESHOLD for d in data_test]  # for nature\n",
    "\n",
    "#y_train = [np.sum(epi_model(d))>THRESHOLD for d in traindist]  ## labels for human human data\n",
    "#y_test = [np.sum(epi_model(d))>THRESHOLD for d in testdist] # for human human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanma\\AppData\\Local\\Temp\\ipykernel_16940\\3684963134.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  filelist=np.array(filelist)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m epidemiologic_models \u001b[39m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mlambda\u001b[39;00m x : \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39marray(x),\n\u001b[0;32m      7\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mbox\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mlambda\u001b[39;00m x : np\u001b[39m.\u001b[39marray(x)\u001b[39m<\u001b[39m\u001b[39m=\u001b[39mcmthresh,\n\u001b[0;32m      8\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mlambda\u001b[39;00m x : (\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mnp\u001b[39m.\u001b[39mexp((np\u001b[39m.\u001b[39marray(x)\u001b[39m-\u001b[39mcmthresh)\u001b[39m/\u001b[39m\u001b[39m30\u001b[39m))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtanhyp\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mlambda\u001b[39;00m x : np\u001b[39m.\u001b[39mtanh((np\u001b[39m.\u001b[39marray(x)\u001b[39m-\u001b[39mcmthresh)\u001b[39m/\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     11\u001b[0m mlmodel_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlogisticreg\u001b[39m\u001b[39m\"\u001b[39m: LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, tol\u001b[39m=\u001b[39m\u001b[39m1e-10\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39msupportvec\u001b[39m\u001b[39m\"\u001b[39m: svm\u001b[39m.\u001b[39mSVC(probability\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     12\u001b[0m \u001b[39m'\u001b[39m\u001b[39mlda\u001b[39m\u001b[39m'\u001b[39m:LinearDiscriminantAnalysis(), \u001b[39m'\u001b[39m\u001b[39mdtree\u001b[39m\u001b[39m'\u001b[39m:DecisionTreeClassifier() ,\u001b[39m'\u001b[39m\u001b[39mknn\u001b[39m\u001b[39m'\u001b[39m:KNeighborsClassifier()}\n\u001b[1;32m---> 14\u001b[0m dataset_load \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnature\u001b[39m\u001b[39m\"\u001b[39m: nature_data(), \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m: human_human()}\n",
      "Cell \u001b[1;32mIn[89], line 13\u001b[0m, in \u001b[0;36mhuman_human\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m traindata\u001b[39m=\u001b[39m[]\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(pathlist)):\n\u001b[1;32m---> 13\u001b[0m     datasetfile\u001b[39m=\u001b[39mfilelist[:,\u001b[39m2\u001b[39;49m][i]\n\u001b[0;32m     14\u001b[0m     pathfile\u001b[39m=\u001b[39mpathlist[i]\n\u001b[0;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(pathfile\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdatasetfile, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mANSI\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "epidemiologic_models = {\n",
    "  \"linear\" : lambda x : 1/np.array(x),\n",
    "  \"box\" : lambda x : np.array(x)<=cmthresh,\n",
    "  \"sigmoid\" : lambda x : (1+np.exp((np.array(x)-cmthresh)/30))**(-1),\n",
    "    \"tanhyp\":lambda x : np.tanh((np.array(x)-cmthresh)/30)\n",
    "}\n",
    "mlmodel_dict = {\"logisticreg\": LogisticRegression(max_iter=10000, tol=1e-10), \"supportvec\": svm.SVC(probability=True),\n",
    "'lda':LinearDiscriminantAnalysis(), 'dtree':DecisionTreeClassifier() ,'knn':KNeighborsClassifier()}\n",
    "\n",
    "dataset_load = {\"nature\": nature_data(), \"human\": human_human()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# 'human' , 'nature'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cmthresh,reflen,testdist,traindist,X_test,X_train\u001b[39m=\u001b[39mdataset_load[dataset]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_load' is not defined"
     ]
    }
   ],
   "source": [
    "dataset='human' # 'human' , 'nature'\n",
    "cmthresh,reflen,testdist,traindist,X_test,X_train=dataset_load[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmthresh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m EPIDEMIOLOGICAL_MODEL \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#\"linear\",\"sigmoid\" ,\"tanhyp\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m epi_model \u001b[39m=\u001b[39m epidemiologic_models[EPIDEMIOLOGICAL_MODEL]\n\u001b[1;32m----> 5\u001b[0m REFERENCE_SEQUENCE \u001b[39m=\u001b[39m [cmthresh]\u001b[39m*\u001b[39mreflen\n\u001b[0;32m      6\u001b[0m \u001b[39m# Compute the THRESHOLD value by inserting the reference sequence into the epidemiological model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m THRESHOLD \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(epi_model(REFERENCE_SEQUENCE))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cmthresh' is not defined"
     ]
    }
   ],
   "source": [
    "# histogram epidemic model risks across threshold\n",
    "\n",
    "EPIDEMIOLOGICAL_MODEL =\"sigmoid\" #\"linear\",\"sigmoid\" ,\"tanhyp\"\n",
    "epi_model = epidemiologic_models[EPIDEMIOLOGICAL_MODEL]\n",
    "REFERENCE_SEQUENCE = [cmthresh]*reflen\n",
    "# Compute the THRESHOLD value by inserting the reference sequence into the epidemiological model\n",
    "THRESHOLD = np.sum(epi_model(REFERENCE_SEQUENCE))\n",
    "# Label data points as \"dangerous\" or not by applying the epidemiological model and comparing with the reference sequence\n",
    "y_train = [np.sum(epi_model(d))>THRESHOLD for d in traindist]  ## labels \n",
    "y_test = [np.sum(epi_model(d))>THRESHOLD for d in testdist] ## \n",
    "plt.hist(list(map(int,y_train)), density=True)\n",
    "plt.title('train_threshold_'+str(cmthresh)+'_'+EPIDEMIOLOGICAL_MODEL)\n",
    "plt.savefig('f:/junaid/pdf_train')\n",
    "plt.show()\n",
    "'''\n",
    "plt.hist(list(map(int,y_test)), density=True)\n",
    "plt.title('test_threshold_'+str(cmthresh)+'_'+EPIDEMIOLOGICAL_MODEL)\n",
    "plt.savefig('f:/junaid/pdf_test')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmthresh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ifig\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m ml_model \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mlogisticreg\u001b[39m\u001b[39m\"\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39msupportvec\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlda\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdtree\u001b[39m\u001b[39m'\u001b[39m ,\u001b[39m'\u001b[39m\u001b[39mknn\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m    REFERENCE_SEQUENCE \u001b[39m=\u001b[39m [cmthresh]\u001b[39m*\u001b[39mreflen\n\u001b[0;32m      5\u001b[0m    \u001b[39m# Select the epidemiological model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m    epi_model \u001b[39m=\u001b[39m epidemiologic_models[EPIDEMIOLOGICAL_MODEL]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cmthresh' is not defined"
     ]
    }
   ],
   "source": [
    "EPIDEMIOLOGICAL_MODEL =\"sigmoid\" #\"linear\",\"sigmoid\" ,\"tanhyp\"\n",
    "ifig=3\n",
    "for ml_model in [\"logisticreg\" , \"supportvec\",'lda', 'dtree' ,'knn']:\n",
    "   REFERENCE_SEQUENCE = [cmthresh]*reflen\n",
    "   # Select the epidemiological model\n",
    "   epi_model = epidemiologic_models[EPIDEMIOLOGICAL_MODEL]\n",
    "   model=mlmodel_dict[ml_model]\n",
    "   # Compute the THRESHOLD value by inserting the reference sequence into the epidemiological model\n",
    "   THRESHOLD = np.sum(epi_model(REFERENCE_SEQUENCE))\n",
    "   # Label data points as \"dangerous\" or not by applying the epidemiological model and comparing with the reference sequence\n",
    "   y_train = [np.sum(epi_model(d))>THRESHOLD for d in traindist]  ## labels for human human data\n",
    "   # Only rssi data is used for testing\n",
    "   y_test = [np.sum(epi_model(d))>THRESHOLD for d in testdist] # for human human\n",
    "   # Fit the model to the data\n",
    "   model.fit(extract_features(X_train), y_train)\n",
    "   # Compute probabilities of dangerous contact using our pretrained model\n",
    "   y_test_pred_proba = model.predict_proba(extract_features(X_test))\n",
    "   # Label data points as \"dangerous\" or not by applying the epidemiological model and comparing with the reference sequence\n",
    "   fpr, tpr, thresholds=roc_curve(y_test, y_test_pred_proba[:,1])\n",
    "   auc=str(round(roc_auc_score(y_test, y_test_pred_proba[:,1]),2))\n",
    "   plt.figure(1)\n",
    "   plt.plot(fpr,tpr,label=ml_model+'_AUC_'+auc)\n",
    "   plt.title('ROC '+dataset)\n",
    "   plt.legend()\n",
    "   plt.savefig('f:/junaid/roc'+dataset)\n",
    "   #plt.show()\n",
    "   f1socres=[]\n",
    "   CLASSIFIER_THRESHOLDS =[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "   for CLASSIFIER_THRESHOLD in CLASSIFIER_THRESHOLDS:\n",
    "    f1s=round(f1_score(y_test, y_test_pred_proba[:,1]>CLASSIFIER_THRESHOLD),2)\n",
    "    f1socres.append(f1s)\n",
    "   plt.figure(2)\n",
    "   plt.plot(CLASSIFIER_THRESHOLDS,f1socres,marker='.',label=ml_model+'_best-fscore'+str(max(f1socres))+'_best-threshold_'+str(CLASSIFIER_THRESHOLDS[f1socres.index(max(f1socres))]))\n",
    "   plt.legend()\n",
    "   plt.title('f_score '+dataset)\n",
    "   plt.savefig('f:/junaid/fscore'+dataset)\n",
    "   with open('f:/junaid/epidemic_results.csv', 'a') as csvfile:\n",
    "     cswres = csv.writer(csvfile)\n",
    "     cswres.writerow(['dataset','EPIDEM_func','ml_model','auc','max_f1socres','best_CLASSIFIER_THRESHOLD','reference_dist_thresh','reference_vec_len'])\n",
    "     cswres.writerow([dataset,EPIDEMIOLOGICAL_MODEL,ml_model,auc,max(f1socres),CLASSIFIER_THRESHOLDS[f1socres.index(max(f1socres))],cmthresh,reflen])###\n",
    "   plt.figure(3+ifig)\n",
    "   plt.hist(np.argmax(y_test_pred_proba,axis=1), density=True)\n",
    "   #plt.savefig('f:/junaid/prdpdf'+str(cmthresh)+str(ifig))\n",
    "   ifig=ifig+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Compute the number Positives, True Positives, Negatives and False Positives \n",
    " TP = np.sum(y_test * (y_test_pred_proba[:,1]>CLASSIFIER_THRESHOLD))\n",
    " P = np.sum(y_test)\n",
    " FP = np.sum((1-np.array(y_test)) * (y_test_pred_proba[:,1]>CLASSIFIER_THRESHOLD))\n",
    " N = np.sum((1-np.array(y_test)) )\n",
    " #print('classifier_thresh',CLASSIFIER_THRESHOLD)\n",
    " #print(\"Positive {}\\nTrue Positive {}\\nNegative {}\\nFalse Positive {}\".format(P, TP, N, FP))\n",
    "# neural net\n",
    " outdata=to_categorical(outdata1)\n",
    " werlabelval=to_categorical(werlabelval1)\n",
    " spl=len(indata)\n",
    " inputdim=len(indata[0])\n",
    " outputdim=len(outdata[0])\n",
    " input_flat1 = Input(shape=(inputdim,))\n",
    " hidden_layer1 = Dense(int((outputdim+inputdim)/2))(input_flat1)\n",
    " h2=BatchNormalization()(hidden_layer1)\n",
    " h3=PReLU()(h2)\n",
    " h4=Dropout(0.1)(h3)\n",
    " output_layer1 = Dense(outputdim, activation='softmax')(h4)\n",
    " emodel1 = Model(input_flat1,output_layer1)\n",
    " emodel1.compile(optimizer='Nadam', loss='categorical_crossentropy')\n",
    " emodel1.summary()\n",
    " #plot_model(emodel1, to_file='dmodel.png')\n",
    " es = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience=dpt)\n",
    " hist= emodel1.fit(indata,outdata,epochs=epc,batch_size=32,shuffle=True,validation_split=dvspt,callbacks=[es])\n",
    " flat_outa = emodel1.predict(werval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
